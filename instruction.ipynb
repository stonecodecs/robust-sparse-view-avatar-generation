{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEVA pipeline introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For inference\n",
    "Mainly command line based, calling functions from `preprocessing.py`.\\\n",
    "**Note:** we don't use the cropping dataloader yet, and all the cropping logistics are still 1024x1024 deterministic squares.\\\n",
    "(TODO: Use the `RandomBBoxCrop` class from `dataloader.py`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1803576428.py, line 20)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m--apply_split_only\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# first, ensure that preprocessing.py is in the current directory\n",
    "# !python preprocessing.py -h\n",
    "\n",
    "# example use to get a multi-view static scene from mvhumannet dataset ready for SEVA inference of a SINGLE subject\n",
    "# required: base_dir, timestep, output_dir\n",
    "!python preprocessing.py \\\n",
    "    --base_dir \"path_to_mvhumannet_dataset_subject\" \\\n",
    "    --timestep 5 \\\n",
    "    --output_dir \"output_path_to_store\" \\\n",
    "    --seconds 3 \\\n",
    "    --fps 10\n",
    "    # --num_train_frames 0 # no test frames\n",
    "    # --train_ids_path \"path_to_train_ids_txt\" # is a txt file in which you can specify the exact training frames to use\n",
    "# notes:\n",
    "# - currently, --subject_id not used, but can change easily; otherwise, just defined subject path as above\n",
    "# - if you DON'T want to generate orbital path poses (only train/test poses), set num_train_frames = 0 (overwriting fps & seconds)\n",
    "# - black frames are generated as \"placeholders\" for the missing views for our custom orbital path. This is normal.\n",
    "# - if you want to post-process all transform matrices with another, use --transform_coords (with a txt file from np.savetxt)\n",
    "# - --crop_only is deprecated (probably don't want to use)\n",
    "\n",
    "# generation also may take some time\n",
    "# if you want to generate different train_test_splits_{num_train_frames}.json, use --apply_split_only boolean tag\n",
    "# this will skip the generation process and simply generate the json of the order of train and test poses\n",
    "!python preprocessing.py \\\n",
    "    --base_dir \"path_to_mvhumannet_dataset_subject\" \\\n",
    "    --timestep 5 \\\n",
    "    --output_dir \"output_path_to_store\" \\\n",
    "    --seconds 3 \\\n",
    "    --fps 10 \\\n",
    "    --apply_split_only\n",
    "# this shouldn't redo any computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera scale file not found at demo_inputs/assets_demo_cli/garden_flythrough/camera_scale.pkl. Using default scale of 1.0. This is fine if transforms have already been scaled.\n",
      "Figure(1000x1000)\n"
     ]
    }
   ],
   "source": [
    "# camera pose visualization\n",
    "\n",
    "# run this in actual command line, otherwise it won't pop up\n",
    "!python visuals.py --transforms_path \"demo_inputs/assets_demo_cli/garden_flythrough/transforms.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining tests: 11\n",
      "Ignoring input image: ./input/000.png\n",
      "IMAGE COMPARISON STATS:\n",
      "\n",
      "11 (360, 640, 3)\n",
      "11 (576, 576, 3)\n",
      "11 (3, 4)\n",
      "Target dimensions: 576x576\n",
      "Initial shapes: GT (360, 640, 3), Generated (576, 576, 3)\n",
      "After scaling: GT (576, 576, 3), Generated (576, 576, 3)\n",
      "Saving video...\n",
      "Saving video...: 100%|██████████████████████████████████████████████████████████████████| 11/11 [00:00<00:00, 11.08it/s]\n",
      "Video saved as comparison_re10k.mp4\n"
     ]
    }
   ],
   "source": [
    "# view comparison (somewhat bad design: it uses the same file but --comparison flag for a different visualization)\n",
    "!python visuals.py \\\n",
    "    --comparison \\\n",
    "    --gt_dir \"to_processed_input_dir\" \\\n",
    "    --comparison_dir \"to_processed_output_dir\" \\\n",
    "    --num_split 9 \\\n",
    "    --output_path \"rendered_video_output_path\" \\\n",
    "    --fps 30\n",
    "\n",
    "# NOTE: resolutions are assumed to be square, otherwise this won't really work well.\n",
    "# --gt_dir is our generated input directory (MVHumanNet subject directory with \"transforms.json\")\n",
    "# --comparison_dir is the directory of the outputs with the SEVA outputs \"first-pass, input, samples-rgb, transforms.json\" directories and files.\n",
    "# --num_split is important, and there must be a corresponding train_test_split_{num_split}.json file in the --gt_dir directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "From `dataloader.py`, cropping class & dataloader.\\\n",
    "(these are dependent on `preprocessing.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stone/dev/playground/seva_preprocessing/.venv/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as T\n",
    "from PIL import Image\n",
    "from preprocessing import get_bbox_center_and_size, get_mvhumannet_extrinsics, load_json, load_pickle, update_intrinsics_resize, generate_gaussian_mixture_samples, generate_gaussian_samples\n",
    "import json\n",
    "from dataloader import MVHumanNetDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize(576), # whatever final resolution we want here\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "mvhumannet_dataset = 'mvset/' # change this to dataset path, with the structure as below:\n",
    "# Expected dataset structure:\n",
    "# ${mvhumannet_dataset}/\n",
    "# ├── subject_id1/\n",
    "# │   ├── annots/\n",
    "# │   │   ├── camera1/\n",
    "# │   │   │   └── frame_001.json\n",
    "# │   │   └── camera2/\n",
    "# │   │       └── frame_001.json\n",
    "# │   ├── images_lr/\n",
    "# │   │   ├── camera1/\n",
    "# │   │   │   └── frame_001.jpg\n",
    "# │   │   └── camera2/\n",
    "# │   │       └── frame_001.jpg\n",
    "# │   ├── fmask_lr/\n",
    "# │   │   ├── camera1/\n",
    "# │   │   │   └── frame_001_fmask.png\n",
    "# │   │   └── camera2/\n",
    "# │   │       └── frame_001_fmask.png\n",
    "# │   ├── camera_extrinsics.json\n",
    "# │   ├── camera_intrinsics.json\n",
    "# │   └── camera_scale.pkl\n",
    "# └── subject_id2/\n",
    "#     └── ...\n",
    "\n",
    "# sampling distributions\n",
    "\n",
    "# 'pre_scale' accounts for the intrinsics-related camera scaling\n",
    "# as MVHumanNet authors downsampled by a factor of 2 beforehand.\n",
    "mvds = MVHumanNetDataset(root_dir=mvhumannet_dataset, transforms=transform, pre_scale=0.5)\n",
    "dataloader_train = DataLoader(mvds, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stone/dev/playground/seva_preprocessing/.venv/lib/python3.11/site-packages/torchvision/transforms/v2/functional/_deprecated.py:12: UserWarning: The function `to_tensor(...)` is deprecated and will be removed in a future release. Instead, please use `to_image(...)` followed by `to_dtype(..., dtype=torch.float32, scale=True)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example samples post-processed cropped images\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m cropped_img, updated_K, transform_matrix = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeature batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcropped_img.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabels batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mupdated_K.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/playground/seva_preprocessing/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/playground/seva_preprocessing/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/playground/seva_preprocessing/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/playground/seva_preprocessing/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/playground/seva_preprocessing/dataloader.py:220\u001b[39m, in \u001b[36mMVHumanNetDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    216\u001b[39m masked_img = Image.composite(img, background, mask)\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# random crop the image\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# random_cropper = RandomBBoxCrop(center_sampler, length_sampler)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m cropped_image, updated_K = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcropper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_img\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintrinsics\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# apply transforms\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/playground/seva_preprocessing/dataloader.py:105\u001b[39m, in \u001b[36mRandomBBoxCrop.__call__\u001b[39m\u001b[34m(self, image, bbox, K)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mself\u001b[39m, \n\u001b[32m     92\u001b[39m     image: torch.Tensor, \n\u001b[32m     93\u001b[39m     bbox: torch.Tensor, \n\u001b[32m     94\u001b[39m     K: torch.Tensor\n\u001b[32m     95\u001b[39m ) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[32m     96\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[33;03m        image: Tensor of shape (C, H, W)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m        Cropped image and updated intrinsics matrix\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     crop_params, K_new = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_crop_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     x1, y1, x2, y2 = crop_params\n\u001b[32m    108\u001b[39m     \u001b[38;5;66;03m# Handle padding if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/playground/seva_preprocessing/dataloader.py:64\u001b[39m, in \u001b[36mRandomBBoxCrop._get_crop_params\u001b[39m\u001b[34m(self, bbox, K)\u001b[39m\n\u001b[32m     61\u001b[39m center_y = (bbox[\u001b[32m1\u001b[39m] + bbox[\u001b[32m3\u001b[39m]) / \u001b[32m2\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# sample new center and length of crop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m center_sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcenter_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     65\u001b[39m size_sample = \u001b[38;5;28mself\u001b[39m.length_sampler(\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# calculate crop coordinates\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mcenter_sampler\u001b[39m\u001b[34m(batch_size)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcenter_sampler\u001b[39m(batch_size):\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# mean at center of bbox\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     mean = torch.tensor([\u001b[43mx1\u001b[49m + size_x // \u001b[32m2\u001b[39m, y1 + size_y // \u001b[32m2\u001b[39m], dtype=torch.float32)\n\u001b[32m     46\u001b[39m     cov = torch.tensor([[size_x, \u001b[32m0\u001b[39m], [\u001b[32m0\u001b[39m, size_y]], dtype=torch.float32)\n\u001b[32m     47\u001b[39m     weights = torch.tensor([\u001b[32m0.7\u001b[39m, \u001b[32m0.3\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'x1' is not defined"
     ]
    }
   ],
   "source": [
    "# Example samples post-processed cropped images\n",
    "cropped_img, updated_K, transform_matrix = next(iter(dataloader_train))\n",
    "print(f\"Feature batch shape: {cropped_img.size()}\")\n",
    "print(f\"Labels batch shape: {updated_K.size()}\")\n",
    "print(f\"Transforms batch shape: {transform_matrix.size()}\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Convert tensor to numpy for plotting\n",
    "img_to_plot = cropped_img.squeeze(0).permute(1, 2, 0).numpy()\n",
    "# Ensure values are in valid range for imshow\n",
    "if img_to_plot.max() <= 1.0:\n",
    "    plt.imshow(img_to_plot)\n",
    "else:\n",
    "    plt.imshow(img_to_plot / 255.0)\n",
    "plt.title('Cropped Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: pull this training branch and get it working: https://github.com/nviolante25/stable-virtual-camera/tree/training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
